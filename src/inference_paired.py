import os
import argparse
import numpy as np
from PIL import Image
import torch
from torchvision import transforms
import torchvision.transforms.functional as F
from pix2pix_turbo import Pix2Pix_Turbo
from image_prep import canny_from_pil
import cv2 # Import OpenCV

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--input_image', type=str, required=True, help='path to the input image')
    parser.add_argument('--prompt', type=str, required=True, help='the prompt to be used')
    parser.add_argument('--model_name', type=str, default='', help='name of the pretrained model to be used')
    parser.add_argument('--model_path', type=str, default='', help='path to a model state dict to be used')
    parser.add_argument('--output_dir', type=str, default='output', help='the directory to save the output')
    parser.add_argument('--low_threshold', type=int, default=100, help='Canny low threshold')
    parser.add_argument('--high_threshold', type=int, default=200, help='Canny high threshold')
    parser.add_argument('--gamma', type=float, default=0.4, help='The sketch interpolation guidance amount')
    parser.add_argument('--seed', type=int, default=42, help='Random seed to be used')
    parser.add_argument('--use_fp16', action='store_true', help='Use Float16 precision for faster inference')
    args = parser.parse_args()

    # only one of model_name and model_path should be provided
    if args.model_name == '' and args.model_path == '':
        raise ValueError('Either model_name or model_path should be provided')

    os.makedirs(args.output_dir, exist_ok=True)

    # initialize the model
    model = Pix2Pix_Turbo(pretrained_name=args.model_name, pretrained_path=args.model_path)
    model.set_eval()
    if args.use_fp16:
        model.half()

    # make sure that the input image is a multiple of 8
    input_image = Image.open(args.input_image).convert('RGB')
    new_width = input_image.width - input_image.width % 8
    new_height = input_image.height - input_image.height % 8
    input_image = input_image.resize((new_width, new_height), Image.LANCZOS)
    bname = os.path.basename(args.input_image)

    # translate the image
    with torch.no_grad():
        if args.model_name == 'edge_to_image':
            # This part is for the original pretrained models, not your fine-tuned one
            canny = canny_from_pil(input_image, args.low_threshold, args.high_threshold)
            canny_viz_inv = Image.fromarray(255 - np.array(canny))
            canny_viz_inv.save(os.path.join(args.output_dir, bname.replace('.png', '_canny.png')))
            c_t = F.to_tensor(canny).unsqueeze(0).cuda()
            if args.use_fp16:
                c_t = c_t.half()
            output_image = model(c_t, args.prompt)

        elif args.model_name == 'sketch_to_image_stochastic':
            # This part is for the original pretrained models, not your fine-tuned one
            image_t = F.to_tensor(input_image) < 0.5
            c_t = image_t.unsqueeze(0).cuda().float()
            torch.manual_seed(args.seed)
            B, C, H, W = c_t.shape
            noise = torch.randn((1, 4, H // 8, W // 8), device=c_t.device)
            if args.use_fp16:
                c_t = c_t.half()
                noise = noise.half()
            output_image = model(c_t, args.prompt, deterministic=False, r=args.gamma, noise_map=noise)

        else:
          # --- THIS IS THE CORRECTED LOGIC FOR THE NEW STRATEGY ---
          
          # 1. Generate the Canny edge map from the input image
          input_cv = np.array(input_image)
          edges_cv = cv2.Canny(input_cv, 100, 200)
          canny_pil = Image.fromarray(edges_cv).convert("RGB")

          # 2. Convert to a tensor
          c_t = F.to_tensor(canny_pil).unsqueeze(0).cuda()

          if args.use_fp16:
              c_t = c_t.half()
          
          # 3. Run inference
          output_image = model(c_t, args.prompt)

    # --- EXPERIMENTAL COMPOSITING STEP ---
    flow_only_pil = transforms.ToPILImage()(output_image[0].cpu() * 0.5 + 0.5)
    input_cv = cv2.imread(args.input_image)
    flow_cv = cv2.cvtColor(np.array(flow_only_pil), cv2.COLOR_RGB2BGR)

    h, w, _ = input_cv.shape
    flow_cv = cv2.resize(flow_cv, (w, h))

    # 1. Isolate the airfoil from the input image
    gray_input = cv2.cvtColor(input_cv, cv2.COLOR_BGR2GRAY)
    _, airfoil_mask = cv2.threshold(gray_input, 10, 255, cv2.THRESH_BINARY)

    # 2. Define the "flow zone" by expanding (dilating) the airfoil shape
    # You can make the kernel larger (e.g., (15,15)) to expand the zone further
    kernel = np.ones((15, 15), np.uint8)
    dilated_mask = cv2.dilate(airfoil_mask, kernel, iterations=20)
    # The flow zone is the area in the dilated mask that is NOT the airfoil itself
    flow_zone_mask = cv2.subtract(dilated_mask, airfoil_mask)

    # 3. Isolate the misplaced flow generated by the model
    gray_flow = cv2.cvtColor(flow_cv, cv2.COLOR_BGR2GRAY)
    _, generated_flow_mask = cv2.threshold(gray_flow, 10, 255, cv2.THRESH_BINARY)
    
    # "Cut" the flow pixels from the generated image
    source_pixels = flow_cv[generated_flow_mask > 0]

    # 4. Create a new image by pasting the airfoil onto a black canvas
    final_image = np.zeros_like(input_cv)
    final_image[airfoil_mask > 0] = input_cv[airfoil_mask > 0]
    
    # 5. "Paste" the cut flow pixels into the defined flow zone
    if len(source_pixels) > 0:
        target_coords = np.argwhere(flow_zone_mask > 0)
        
        # If we have more target spots than source pixels, repeat the source pixels
        num_repeats = int(np.ceil(len(target_coords) / len(source_pixels)))
        pasted_pixels = np.tile(source_pixels, (num_repeats, 1))[:len(target_coords)]
        
        # Shuffle the pixels for a more random placement
        np.random.shuffle(pasted_pixels)
        
        # Place the pixels onto the final image
        final_image[target_coords[:, 0], target_coords[:, 1]] = pasted_pixels

    # --- SAVE THE FINAL COMPOSITE IMAGE ---
    composite_filename = bname.replace('.png', '_composite.png')
    cv2.imwrite(os.path.join(args.output_dir, composite_filename), final_image)
    
    print(f"âœ… Final composite image saved to: {os.path.join(args.output_dir, composite_filename)}")
